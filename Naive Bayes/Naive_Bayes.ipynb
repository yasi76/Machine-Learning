{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Example 1:"
      ],
      "metadata": {
        "id": "7IhRy0sxBjWo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PQjGlCX4boBW",
        "outputId": "fce9a78e-1186-4fa5-d39a-a9b0c9a7997b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.9736842105263158\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Breast cancer dataset)\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the Gaussian Naive Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_nb = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 2:"
      ],
      "metadata": {
        "id": "HKhAswyH_EXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Example dataset\n",
        "data = [\n",
        "    (\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight. I've cried enough today.\", \"ham\"),\n",
        "    (\"Win a $100 Walmart gift card! Enter now\", \"spam\"),\n",
        "    (\"Do you want to grab coffee tomorrow?\", \"ham\"),\n",
        "    (\"URGENT! You have won a 1 week FREE membership in our $100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\", \"spam\"),\n",
        "    (\"Can we meet this Sunday? I miss you\", \"ham\"),\n",
        "    (\"Reminder from O2: To get 2.50 pounds free call credit, go to http://www.o2.co.uk and enter your code\", \"spam\"),\n",
        "    (\"Hey, how are you doing?\", \"ham\"),\n",
        "    (\"Congrats! 1 year special cinema pass for two is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3, etc all 4 FREE! bx420-ip4-5we. 150pm\", \"spam\"),\n",
        "    (\"I'm on my way to the store, do you need anything?\", \"ham\"),\n",
        "    (\"You have WON a guaranteed £1000 cash or a £2000 prize. To claim yours, call our customer service representative at 0800 123 456\", \"spam\"),\n",
        "    (\"It was lovely to see you last night, we should do it again soon.\", \"ham\"),\n",
        "    (\"IMPORTANT - You could be entitled up to £3,900 in compensation from mis-sold PPI on a credit or loan. Reply INFO to stop\", \"spam\"),\n",
        "    (\"Good morning, did you sleep well?\", \"ham\"),\n",
        "    (\"See movie tonight? Check out the latest blockbuster.\", \"ham\"),\n",
        "    (\"Claim your free trial of our diet pills! Lose 20 pounds in just one month!\", \"spam\"),\n",
        "    (\"Your mobile number has been selected for a $2,500 luxury cruise for two! Call now to confirm your prize.\", \"spam\"),\n",
        "    (\"Dinner tonight? My place?\", \"ham\"),\n",
        "    (\"Update: Your Amazon order has shipped. Check your delivery status here.\", \"ham\"),\n",
        "    (\"You're invited! Exclusive event this weekend at our store. Reply YES to RSVP.\", \"spam\"),\n",
        "    (\"Could you help me with the project tomorrow? I could really use your expertise.\", \"ham\"),\n",
        "    (\"Don't miss out on this exclusive offer! Buy one get one free for a limited time only!\", \"spam\"),\n",
        "    (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)\", \"spam\"),\n",
        "    (\"Nah I don't think he goes to usf, he lives around here though\", \"ham\"),\n",
        "    (\"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\", \"spam\"),\n",
        "    (\"Even my brother is not like to speak with me. They treat me like aids patent.\", \"ham\"),\n",
        "    (\"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\", \"spam\"),\n",
        "]\n",
        "\n",
        "# Separate the data and labels\n",
        "texts = [text for text, label in data]\n",
        "labels = [label for text, label in data]\n"
      ],
      "metadata": {
        "id": "SHO_gGuohuQ8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset\n",
        "text_train, text_test, label_train, label_test = train_test_split(texts, labels, test_size=0.33, random_state=42)\n",
        "\n",
        "# Vectorizing the text data\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train = vectorizer.fit_transform(text_train)\n",
        "X_test = vectorizer.transform(text_test)\n",
        "\n",
        "# Training the Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, label_train)\n",
        "\n",
        "# Making predictions\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "# Evaluating the classifier\n",
        "print(f\"Accuracy: {accuracy_score(label_test, predictions)}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(label_test, predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mrCK-Y6_JGP",
        "outputId": "915d3f24-ef24-4eea-8d4f-fea0356a1651"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6666666666666666\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       1.00      0.40      0.57         5\n",
            "        spam       0.57      1.00      0.73         4\n",
            "\n",
            "    accuracy                           0.67         9\n",
            "   macro avg       0.79      0.70      0.65         9\n",
            "weighted avg       0.81      0.67      0.64         9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: predict a sentence\n",
        "new_sentence = \"Congratulations! You've won a free trip to Hawaii!\"\n",
        "new_sentence_vectorized = vectorizer.transform([new_sentence])\n",
        "prediction = classifier.predict(new_sentence_vectorized)[0]\n",
        "print(f\"Prediction for '{new_sentence}': {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7iALIGyBUAn",
        "outputId": "06af7365-c61b-4619-8920-4be8251f19ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for 'Congratulations! You've won a free trip to Hawaii!': spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_sentence_vectorized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvgORMxNs8Tg",
        "outputId": "30b523df-d323-4df4-bea6-aef62540e564"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 43)\t1\n",
            "  (0, 124)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer2 = CountVectorizer()\n",
        "print(vectorizer2.fit_transform([\"hello hello this,  11  ! is a test\", \"this is another test\", \"third example another another test zebra\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meXpzUz4tq8L",
        "outputId": "27f95874-ad50-40d7-cbce-6a15f85fe1b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 3)\t2\n",
            "  (0, 7)\t1\n",
            "  (0, 0)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 5)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 5)\t1\n",
            "  (1, 1)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 1)\t2\n",
            "  (2, 6)\t1\n",
            "  (2, 2)\t1\n",
            "  (2, 8)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: can you map the words to numers in vectorizer2\n",
        "\n",
        "print(vectorizer2.vocabulary_)"
      ],
      "metadata": {
        "id": "Phbd4eIFwe-o",
        "outputId": "94157c02-3f62-4525-8a12-6c8a67384cca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hello': 3, 'this': 7, '11': 0, 'is': 4, 'test': 5, 'another': 1, 'third': 6, 'example': 2, 'zebra': 8}\n"
          ]
        }
      ]
    }
  ]
}